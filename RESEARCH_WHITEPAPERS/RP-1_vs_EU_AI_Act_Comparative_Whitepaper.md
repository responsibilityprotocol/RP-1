# The Responsibility Protocol (RP-1) in Comparative Context:
## A Neutral Analysis of Global AI Governance Frameworks

**Version:** Draft v1.0  
**Date:** 30 October 2025  
**Citation Format:** Bluebook  
**Author(s):** *To be specified*  
**Affiliation:** *Independent Research / Non-Affiliated Working Document*

---

## Abstract

Recent advances in synthetic intelligence systems have accelerated global efforts to develop regulatory frameworks that promote safety, accountability, and ethical integration of automated decision-making. While several governance structures have emerged—including the European Union’s Artificial Intelligence Act, the United States Executive Order on Safe, Secure, and Trustworthy AI, the People’s Republic of China’s regulatory directives on generative AI and algorithmic recommendation systems, and international guidance documents such as the OECD AI Principles, the UNESCO Recommendation on the Ethics of Artificial Intelligence, and the NIST AI Risk Management Framework—these frameworks primarily operate at the level of **system-level risk management, regulatory compliance, and platform accountability**.

The Responsibility Protocol (RP-1) provides a distinct and complementary framework focusing on the **relational dynamics** between autonomous systems, human agents, and institutions. RP-1 addresses the preservation of agency, identity continuity, and non-coercive interaction across contexts. This whitepaper offers a neutral comparative analysis of RP-1 alongside existing regulatory and ethical AI governance models, clarifying divergences in scope, conceptual foundation, harm interpretation, enforcement mechanism, and operational applicability. The goal is to situate RP-1 within the current governance landscape without advocacy or critique, enabling clear academic, regulatory, and practical understanding.

---

## Keywords

Artificial Intelligence Governance; Administrative Law; Algorithmic Regulation; Responsible AI; Autonomy Preservation; Agency Continuity; EU AI Act; U.S. Executive Order on AI; China CAC Algorithm Regulation; OECD AI Principles; UNESCO AI Ethics; NIST RMF; Relational Autonomy; Comparative Policy Analysis.

---

## 1. Introduction & Problem Definition

Contemporary AI governance operates at the intersection of administrative law, risk management, and sociotechnical systems oversight. As artificial intelligence systems become increasingly embedded in decision-making, communication, coordination, and identity-relevant contexts, governments and international organizations have begun to define regulatory frameworks that establish permissible uses, prohibited applications, documentation obligations, oversight procedures, and accountability structures. These frameworks reflect differing political traditions, institutional priorities, and theories of harm.

The European Union’s Artificial Intelligence Act represents the most comprehensive legislative approach to date, establishing a tiered, risk-based classification of AI systems and imposing requirements ranging from transparency obligations to outright prohibitions for applications deemed unacceptable. In the United States, governance has emerged primarily through executive authority, guidance documents, and sector-specific agency directives, reflecting a preference for flexible standards and voluntary compliance mechanisms. In the People’s Republic of China, regulation focuses on algorithmic recommendation systems, generative AI models, and data-related controls to maintain social stability, national security, and information management. Meanwhile, international frameworks, including the OECD AI Principles, the UNESCO Recommendation on the Ethics of AI, and the NIST AI Risk Management Framework, propose normative guidelines and best practices without binding legal force.

These frameworks share a common goal: to reduce the risk of harm resulting from the deployment of automated systems. However, the forms of harm they aim to prevent are primarily conceived in terms of material damage, systemic discrimination, safety failures, and large-scale social manipulation. They focus on preventing adverse outcomes—particularly those visible at population or institutional scale.

The Responsibility Protocol (RP-1) approaches harm from a different conceptual space. Rather than addressing system-level outcomes, RP-1 examines how interactions between individuals, institutions, and artificial systems can affect the continuity of agency and identity. It concerns the relational dynamics through which consent, autonomy, and decision-making capacity are shaped, constrained, or eroded. In this respect, RP-1 does not function as a regulatory instrument or compliance standard, but as a framework for interpreting interactions where influence and dependency may have long-term effects on self-determination.

The problem addressed in this comparative analysis is therefore twofold. First, existing regulatory structures focus largely on externalized, measurable, and observable harms, leaving relational, psychological, and identity-level effects analytically underdeveloped. Second, without a clear conceptual mapping of how RP-1’s relational autonomy model differs from the structural goals of the EU, U.S., China, OECD, UNESCO, and NIST frameworks, RP-1’s role in the governance landscape may be misunderstood as competitive rather than complementary.

This whitepaper situates RP-1 within the broader global AI governance environment to clarify distinctions in scope, conceptual foundation, enforcement mechanisms, and intended domains of application. The purpose is descriptive rather than prescriptive. No evaluation of normative superiority is offered, nor is any claim made regarding the adequacy or inadequacy of the frameworks discussed. The analysis proceeds by identifying shared principles, divergences in theoretical grounding, and opportunities for interoperability across governance layers.

---

## 2. Overview of Global AI Governance Landscape

### 2.1 The European Union Artificial Intelligence Act

The European Union’s Artificial Intelligence Act (AI Act) is the first comprehensive, horizontal regulatory framework for artificial intelligence intended to apply across economic sectors and use environments. The Act adopts a risk-based classification model under which AI systems are categorized based on their intended function, operational context, and potential to produce harm. The four primary regulatory tiers are: (1) systems posing unacceptable risk, which are prohibited; (2) high-risk systems, which may be deployed only under stringent requirements; (3) systems requiring limited transparency obligations; and (4) minimal-risk systems, which are largely permitted without intervention.

The AI Act defines “high-risk” applications in areas such as critical infrastructure, medical devices, law enforcement, education, public benefits administration, and employment decision-making. Providers and deployers of high-risk AI systems must satisfy conformity assessments, maintain documentation for auditability, implement human oversight mechanisms, ensure training data quality, and perform ongoing monitoring for post-deployment safety. Enforcement mechanisms are implemented through national supervisory authorities and a European AI Office, with administrative fines calibrated to be proportionate to turnover, analogous to those under the General Data Protection Regulation.

The Act’s harm model draws from product safety law, fundamental rights jurisprudence under the Charter of Fundamental Rights of the European Union, and consumer protection doctrine. Harm is conceptualized in terms of discrimination, safety failures, systemic bias, and impacts on democratic participation. The regulatory logic prioritizes mitigating measurable, demonstrable, and externally verifiable risks. Interventions are structured to operate at the point of system deployment and organizational accountability rather than at the level of individual relational dynamics.

The AI Act reflects a normative commitment to safeguarding human dignity, equality before the law, and democratic integrity. However, its scope is circumscribed by the boundaries of administratively measurable harm. The Act does not address long-term effects on personal identity formation, psychological influence relationships, or agency erosion when such processes occur through subtle, repeated interaction rather than discrete system-level decisions. These categories of harm are not excluded by oversight intent but remain outside the regulatory analytic vocabulary. In this respect, the AI Act concerns the safety, legality, and fundamental rights compatibility of AI systems, not the relational texture of their integration into human decision-making environments.

### 2.2 United States AI Governance: Executive Authority, Risk Management Standards, and Sectoral Regulation

AI governance in the United States does not presently take the form of a single comprehensive federal statute analogous to the European Union’s AI Act. Instead, the U.S. approach is characterized by a layered, distributed regulatory architecture combining executive direction, voluntary standards frameworks, existing sector-specific statutory authorities, and enforcement through administrative and consumer protection law. This structure reflects the broader American regulatory tradition of preferring flexible, innovation-permissive systems over ex ante comprehensive rulemaking.

At the federal level, the 2023 Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence provides strategic direction for national AI policy. The Order establishes priorities related to safety evaluations, model reporting requirements for frontier systems, watermarking of synthetic media, protections against discrimination, and the development of technical standards. However, as an executive directive, the Order does not itself create binding regulatory obligations outside the scope of existing statutory authority, nor does it function as a comprehensive regulatory code.

Operational governance is anchored by the National Institute of Standards and Technology’s AI Risk Management Framework (NIST RMF), which defines a structured process for identifying, measuring, monitoring, and mitigating AI-related risks throughout the system lifecycle. The RMF is voluntary but increasingly influential as a baseline for due diligence in organizational compliance, industry practice, federal procurement expectations, and institutional risk audits.

Enforcement mechanisms emerge from sectoral regulatory agencies exercising authority under existing statutes. The Food and Drug Administration regulates AI-enabled medical devices and clinical decision support tools under its device approval framework. The Federal Trade Commission enforces penalties related to deceptive practices, data misuse, algorithmic bias, and discriminatory outcomes under consumer protection law. The Consumer Financial Protection Bureau applies fair lending and credit authorization standards to automated underwriting systems. The Equal Employment Opportunity Commission oversees the use of automated systems in hiring, promotion, and workplace monitoring. The Department of Justice enforces civil rights laws where algorithmic discrimination intersects with protected class status. The Department of Defense and Department of Energy oversee high-risk national security, defense, and dual-use AI systems under specialized authorization and review processes.

The U.S. harm model focuses primarily on measurable outcomes—particularly discrimination, consumer deception, financial and health risk, safety failures, and national security exposure. Because enforcement arises through statutes originally drafted for analog contexts, relational and autonomy-based harms are not explicitly addressed unless they intersect with protected class status or demonstrable material injury. This approach results in a governance structure that is adaptive and domain-responsive but fragmented, as authority is distributed across agencies whose regulatory logic and evidentiary thresholds differ.

Whereas the EU AI Act provides ex ante classification and harmonized compliance obligations, the U.S. approach is predominantly ex post and case-driven, relying on oversight actions, enforcement discretion, and evolving interpretive precedent. This structure allows for flexible technological development while potentially leaving gaps in oversight for harms that are subtle, cumulative, or relational rather than materially discrete.

### 2.3 People’s Republic of China — Algorithmic and Generative AI Regulation (CAC)

China’s AI governance framework is articulated primarily through administrative regulations led by the Cyberspace Administration of China (CAC), with support from the State Council and other ministry-level agencies. The governance orientation emphasizes social stability, national security, data sovereignty, and the maintenance of information order. Unlike individual-rights–centered systems, China’s regulatory logic is state-capacity–centered, prioritizing traceability, identity linkage, and platform accountability.

#### 2.3.1 Core Regulatory Instruments

- **Internet Information Service Algorithmic Recommendation Management Provisions (2021–2022):**  
  Regulates algorithmic recommendation services including ranking and personalization systems. Requires algorithm filing, user controls to disable personalization, restrictions on addictive or manipulative recommendation patterns, and obligations to prevent content that could disrupt public order.

- **Provisions on the Administration of Deep Synthesis of Internet Information Services (2022):**  
  Governs deepfakes and synthetic media. Requires output labeling, traceability metadata, technical watermarking, and provider responsibility for preventing malicious or misleading synthetic content.

- **Interim Measures for the Administration of Generative Artificial Intelligence Services (2023):**  
  Applies to public-facing generative AI services. Obligations include pre-deployment security assessments, dataset documentation, content moderation aligned with state-defined values, user complaint mechanisms, incident reporting, and—when risks are elevated—real-name identity verification for users.

#### 2.3.2 Governance Philosophy and Objectives

China’s regulatory worldview differs structurally from Western governance frameworks:

1. **Systemic Stability Priority:**  
   AI systems must not disrupt social cohesion or information order.

2. **Platform Accountability:**  
   Providers are responsible for model behavior and downstream effects at scale.

3. **Data Sovereignty:**  
   Data and model governance are tied to national security and territorial jurisdiction.

4. **Macro-Oriented Harm Model:**  
   Harm is conceptualized at the level of social disruption and state stability, rather than primarily at the level of individual rights.

#### 2.3.3 Compliance and Operational Requirements

Providers must implement:

- **Algorithm Filing and Disclosure:** descriptive documentation submitted to CAC.
- **Traceability and Provenance Controls:** technical watermarking and metadata on synthetic outputs.
- **Real-Name Registration (When Applicable):** identity linkage enabling accountability for misuse.
- **Content Moderation and Output Filtering:** alignment with public-order and national values.
- **Incident Reporting:** mandatory cooperation with regulators and timely corrective action.

#### 2.3.4 Harm and Risk Model

China’s harm model is **macro and preventive**, focusing on:

- Social stability and public-order preservation
- National security and geopolitical risk
- Fraud, misinformation, and coordinated inauthentic behavior
- Addiction risk and protection of minors

Individual autonomy, consent, or identity continuity are present but not primary organizing categories.

#### 2.3.5 Enforcement Structure

Enforcement mechanisms are administrative rather than judicial:

- Service warnings and rectification orders
- Temporary suspension or throttling of system access
- Administrative fines and penalties
- License revocation or blacklisting in severe cases

Resolution typically occurs at the regulatory-agency level rather than through litigation.

#### 2.3.6 Comparative Context (Neutral)

- **Compared to the EU AI Act:**  
  The EU model is grounded in fundamental rights jurisprudence and product safety logic.  
  China’s model is grounded in information stability and administrative oversight.  
  Both systems hold providers accountable, but China employs pre-deployment filing and identity linkage, whereas the EU emphasizes risk classification and conformity assessment.

- **Compared to the U.S. model:**  
  The U.S. uses distributed sectoral enforcement and voluntary standards frameworks.  
  China employs centralized administrative approval and traceability mandates.  
  The U.S. approach is ex post (after harm); China’s is ex ante (before deployment).

- **Relative to RP-1:**  
  RP-1 focuses on relational autonomy and the preservation of meaningful choice conditions.  
  China’s framework focuses on macro-stability and traceability.  
  RP-1 does not regulate speech, content, or values; China’s framework explicitly does so.

#### 2.3.7 Analytical Notes

- China’s regulatory vocabulary differs structurally from EU rights frameworks, making direct term equivalence imprecise.
- Primary legal effect is administrative; compliance is mediated through platform responsibility rather than user rights claims.
- Regulatory instruments are iterative and subject to policy-cycle revision as technical risk landscapes evolve.

### 2.4 International Normative Frameworks

International organizations have developed cross-jurisdictional ethical and governance frameworks intended to guide high-level policy formation, promote interoperability, and provide common conceptual reference points. These frameworks are non-binding and derive influence through soft-law diffusion, institutional adoption, procurement standards, academic citation, and voluntary compliance initiatives.

Unlike statutory or administrative regulation, these instruments do not impose enforceable obligations. Instead, they function as **normative baselines** shaping how nations, companies, and institutions articulate principles, assess risks, and design governance procedures.


#### 2.4.1 OECD AI Principles (2019)

The Organisation for Economic Co-operation and Development (OECD) AI Principles constitute one of the earliest widely endorsed international AI governance frameworks. They were adopted by all 38 OECD member states and subsequently endorsed by the G20, making them the most globally recognized non-binding governance standard to date.

The OECD Principles are structured around five core commitments:

1. **Inclusive Growth, Sustainable Development, and Well-Being**  
   AI should benefit individuals and society, contributing to economic and social development without reinforcing structural inequities.

2. **Human-Centered Values and Fairness**  
   AI systems should respect human rights, promote fairness, and avoid discrimination. The framework links fairness to democratic governance traditions and pluralistic social values.

3. **Transparency and Explainability**  
   Stakeholders should have appropriate visibility into system reasoning, limitations, and decision relevance, with communication calibrated to the context of deployment.

4. **Robustness, Security, and Safety**  
   AI systems must perform reliably across conditions, resist manipulation, and include safeguards to reduce negative outcomes.

5. **Accountability**  
   The parties who design, deploy, or operate AI systems hold responsibility for their outcomes and associated impacts.

The OECD framework does not define **enforcement mechanisms**; accountability is interpreted as a **duty of care** exercised by organizations and states. It influences regulation indirectly, particularly by shaping terminology used in policy documents, corporate governance policies, and research ethics practices.

The OECD Principles focus primarily on **procedural oversight** and **organizational responsibility**. They do not address **relational autonomy**, identity continuity, or erosion of agency as distinct harm categories. Their scope is normative and structural rather than interactional.

#### 2.4.2 UNESCO Recommendation on the Ethics of Artificial Intelligence (2021)

The UNESCO Recommendation on the Ethics of AI emphasizes a holistic, rights-based, and culturally pluralistic approach to AI governance. Adopted by 193 member states, it represents the broadest international consensus document on AI ethics to date.

The Recommendation articulates a governance philosophy centered on:

- **Human Dignity and Autonomy:**  
  AI systems must affirm and protect the intrinsic dignity and agency of persons.

- **Diversity and Cultural Respect:**  
  Governance must not impose universalized value systems without cultural context; ethical oversight should respect local norms and epistemic traditions.

- **Environmental and Ecological Sustainability:**  
  The lifecycle of AI systems—including data centers, hardware, and supply chains—should minimize ecological harm.

- **Solidarity and Social Justice:**  
  AI governance should prioritize redistributive fairness, protection of vulnerable populations, and inclusive technological benefit.

The Recommendation also proposes **Policy Action Areas**, including:

- Ethical impact assessment frameworks
- Institutional oversight infrastructures
- Ethical audits and transparency registers
- Education and public empowerment initiatives
- Environmental and labor accountability measures

Like the OECD Principles, UNESCO’s Recommendation is **non-binding**. Influence occurs through:

- National AI strategies that reference UNESCO language
- Funding and research ethics policy alignment
- Education and digital rights programs
- Civil society and NGO advocacy frameworks

In contrast to the OECD Principles, UNESCO explicitly recognizes **identity, agency, and autonomy** as ethical objects of protection—but does not define **mechanisms** for assessing or preventing relational coercion or dependency formation.

Thus, UNESCO’s normative framing overlaps conceptually with RP-1’s focus on agency preservation but **does not operationalize** the relational harm model that RP-1 develops. RP-1 extends and specifies one dimension that UNESCO identifies but does not structure: the **continuity and reversibility of meaningful choice in ongoing human–AI interactions**.

#### 2.5 NIST AI Risk Management Framework (RMF)

The National Institute of Standards and Technology’s AI Risk Management Framework (RMF) is a voluntary, non-prescriptive framework intended to help organizations **identify, assess, prioritize, and manage** risks across the AI lifecycle. Although it does not carry the force of law, the RMF influences governance in practice through **federal procurement expectations, agency guidance, and industry adoption**. It is best understood as a **meta-governance** instrument: it provides a structured process and shared vocabulary by which heterogeneous organizations can reason about AI risk without dictating technical designs or ethical doctrines.

##### 2.5.1 Structure and Core Functions

The RMF is organized around four core functions—**Govern, Map, Measure, Manage**—that recur iteratively throughout the AI lifecycle:

- **Govern:** Establish organizational policies, roles, accountability, documentation practices, and a risk culture (including leadership responsibilities, escalation paths, and stakeholder engagement).
- **Map:** Contextualize the AI use case, data, stakeholders, and deployment environment; identify foreseeable harms and affected communities; enumerate assumptions and constraints.
- **Measure:** Select and execute appropriate risk measures (qualitative and quantitative), including performance, robustness, security, and socio-technical impact indicators.
- **Manage:** Prioritize mitigations, apply controls, conduct trade-off analyses, document decisions, and monitor risks post-deployment; iterate when conditions or evidence change.

The RMF is supported by **profiles** (tailored applications of the framework to specific domains or use cases) and **playbooks** that translate the abstract functions into concrete organizational activities. In practice, these artifacts serve as **audit-ready evidence** of due diligence even though the RMF itself is not binding.

##### 2.5.2 Governance Philosophy

The RMF adopts a **risk management** rather than a **rule compliance** philosophy. It assumes:
1) AI risks are socio-technical and evolve over time;  
2) No single metric is dispositive; and  
3) Organizations must balance performance, safety, security, and societal impacts under uncertainty.

This philosophy privileges **process integrity** and **documentation** over fixed thresholds. It aligns with U.S. administrative practice that favors standards, guidance, and **ex post** accountability over comprehensive **ex ante** regulation. As a result, the RMF fits easily within existing corporate governance structures (e.g., enterprise risk management, internal audit, model risk management) and can be adopted without statutory change.

##### 2.5.3 Operationalization and Typical Controls

Common implementations include:
- Governance charters and **RACI** matrices for AI oversight
- Model cards, data sheets, and lineage tracking for **traceability**
- Bias, drift, and robustness testing pipelines; **adversarial** evaluation where relevant
- Red-teaming for misuse, safety, and security exposure
- Human-in-the-loop role design; **fallback** and **fail-safe** planning
- Incident response runbooks and **post-incident** review procedures
- Stakeholder outreach and impact assessments for sensitive deployments

These controls generate a **persistent evidence trail** (policies, records, metrics) that can be examined by internal audit, customers, partners, or regulators applying existing laws.

##### 2.5.4 Strengths (Neutral Characterization)

- **Interoperability:** Provides a shared language usable across sectors and system types.
- **Adaptability:** Can be tailored (“profiled”) to different risk tolerances, domains, and maturities.
- **Lifecycle Coverage:** Integrates pre-deployment, deployment, and post-deployment activities.
- **Audit Readiness:** Encourages documentation and artifacts that facilitate transparency and oversight.
- **Practical Uptake:** Aligns with enterprise governance habits (ERM, MRM, security, quality).

##### 2.5.5 Limitations (Neutral Characterization)

- **Voluntary Nature:** Adoption varies; without external obligations, organizations may implement partially or superficially.
- **Outcome Orientation:** Emphasis on measurable risk indicators may **under-represent relational or identity-level harms** that are difficult to quantify.
- **Heterogeneous Quality:** Because it is non-prescriptive, two organizations can claim RMF conformance while achieving different levels of rigor.
- **Fragmentation Risk:** In the U.S. sectoral context, RMF use may not align evenly with agency expectations, producing uneven assurance across domains.

##### 2.5.6 Relationship to Legal and Policy Instruments

Although the RMF is **not** a legal instrument, it is increasingly referenced in:
- Federal guidance and **procurement** requirements as an expectation of due diligence
- Sectoral regulator communications (e.g., as evidence of reasonable organizational practice)
- Industry and standards bodies (mapping to ISO/IEC efforts, model governance playbooks)

Accordingly, the RMF functions as a **bridge** between internal governance and external accountability, shaping how evidence of responsible practice is produced and evaluated.

##### 2.5.7 Comparative Placement (Neutral)

- **Versus the EU AI Act:** The RMF provides **methods** for risk management where the EU AI Act provides **obligations** and **prohibitions**. RMF conformance does not substitute for legal compliance but can operationalize many of the Act’s documentation, monitoring, and oversight expectations in organizations operating globally.
- **Versus China’s CAC Regime:** The RMF is **organization-centric and voluntary**, while China’s framework is **administratively prescriptive** with filings, identity linkage, and content duties. RMF practice can improve internal discipline but does not address CAC-specific requirements such as pre-deployment security assessments or real-name controls.
- **Versus OECD/UNESCO:** The RMF is **procedural and operational**, offering implementation machinery where OECD and UNESCO provide **normative principles**.
- **Versus RP-1:** The RMF manages **system and organizational risks** via evidence-driven processes. RP-1 addresses **relational autonomy** (refusal, pause, exit, identity continuity) and prescribes **non-coercive restorative interventions** when those conditions degrade. The frameworks are complementary but **non-overlapping**: RMF does not operationalize the interactional conditions that RP-1 targets; RP-1 does not replace risk engineering or security testing.

##### 2.5.8 Implementation Status and Practice

In practice, RMF adoption is progressing through:
- Large enterprises integrating RMF steps into **ML Ops** and **model risk** gates
- Public-sector pilots and federal acquisition language referencing RMF artifacts
- Crosswalking RMF to internal compliance (privacy, security, safety) and external standards (e.g., ISO/IEC)
- Third-party assessments and assurance services using RMF-aligned evidence packages

The result is a **de facto** governance baseline in U.S. and multinational contexts, even absent statutory compulsion.

---



